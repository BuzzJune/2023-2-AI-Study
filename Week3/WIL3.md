컴퓨터 비전 - 오감능력 중 가장 많이 뇌에서 처리하는 정보인 시각정보를 처리

사람의 시각 <-> computer vision
장면에 대한 분석 (컴퓨터 그래픽, 렌더링)

Image classification
classifier - 이미지를 라벨에 맵핑, 매칭하는거
input -> classifier -> output
컴퓨터는 이미지를 거대한 숫자 배열로 봄 -> semantic gap

고양이 사진 -> 조명, 각도, 포즈, 위치 변화함 --> 모든 변화에도 classification 가능해야함

이미지 분류 정의
고급 코드 규칙을 작성 - 명시적인 시도 -> 부서지기 쉬움, 확장성X(모든것을 정의 단계부터 해야함) --> 데이터기반 알고리즘으로 해결
인터넷으로 대규모 데이터셋을 수집 -- 기계학습 분류기 훈련

KNN (classifier)
K개에 가장 가까운 것들. 쫌 멍청함
모든 훈련 데이터를 기억. 훈련단계에서 보았던 데이터중 가장 유사한 이미지를 찾아 그 이미지의 라벨로 예측
데이터 셋들의 거리 (RGB 3채널의 픽셀값으로)
가운데 노이즈 -> 다수결의 법칙으로 결정 - 일반화
Manhattan Distance / Euclidean Distance
다양한 거리 측정법 사용 -> 다양한 유형의 데이터를 input값으로 받아올 수 있음 (이미지, 벡터 말고도)
- 확장성 좋음

L1 distance - 경계선이 x,y좌표축을 따름
L2 distance - 좌표축 신경쓰지 않고 자연스럽게 떨어짐

KNN -> 이미지에 쓰지 않음
1. 시간이 오래걸림 / 2. 이미지간의 지각적 유사성 != 픽셀 간의 거리 (이미지에 좋은 방법이 아님)
3. 차원의 저주 때문 (데이터 공간을 채우려면 많은 데이터가 필요. 데이터가 없는데도)

Linear Classification 선형
신경 네트워크에 많이 사용
Image Captioning - 컨볼루션 신경망(이미지를 보는) + 순환 신경망(언어를 아는)

Parametric (KNN과는 다른 접근방식)
f(x,W) = Wx + b
가중치 W, 이미지 x, 편향 b

말의 머리가 2개처럼 보임 -> 하나의 template으로 학습하기 때문 -> 문제점들이 생김
- Non-linear 함수와 같이 써야함.

손실함수 Loss Function
Loss 적으면 = 분류기가 좋다
Loss 많으면 = 분류기가 별로다

SVM Loss
절대적인 예측점수보다 상대적인 높은 값이 정답레이블임이 더 중요

L2 Regularization - 가장 많이 씀. 더 매끄러운걸 고르기 위한

KNN
